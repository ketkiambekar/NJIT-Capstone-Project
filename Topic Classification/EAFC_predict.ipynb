{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EAFC_predict.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketkiambekar/NJIT-Capstone-Project/blob/main/Topic%20Classification/EAFC_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoZQZuRA_ikQ"
      },
      "source": [
        "text=\"I received my result yesterday and my cancer is in remission, yay!!\""
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2rGgPs41XG0",
        "outputId": "12899cf0-d073-43c7-af22-c940644fc0c5"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.45)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJyJLaFvaZRK"
      },
      "source": [
        "#from sklearn.svm import LinearSVC\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "import string\n",
        "import contractions\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#import text_laundry\n"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF6tAt96LgXj",
        "outputId": "6ff11f2f-97ba-409f-c785-9a3b8a8815af"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjxPOv22UabD"
      },
      "source": [
        "def remove_punctuations(text):\n",
        "    punc = string.punctuation \n",
        "    for punctuation in punc:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJixi2Q5v1Uv"
      },
      "source": [
        "def remove_contractions(text):\n",
        "  return \" \".join([contractions.fix(word) for word in text.split()])"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WvaO9Gw3S8O"
      },
      "source": [
        "def remove_stopwords(text, filename):\n",
        "  stop_words=[]\n",
        "  with open(filename,'r') as f:\n",
        "    for line in f:\n",
        "      for word in line.split():\n",
        "        stop_words.append(word.lower()) \n",
        "  result =  [word for word in text if word not in stop_words]\n",
        "  return result"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBegyp53LWN4"
      },
      "source": [
        "\n",
        "# text=\"I am in remission yay\"\n",
        "# tokens= word_tokenize(text)\n",
        "# print(tokens)\n",
        "# tokens = remove_stopwords(tokens, 'stopwords.txt')\n",
        "\n",
        "# print(tokens)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dOEx7rL4cCl"
      },
      "source": [
        "def replace_strings(text, new_string , remove_arr ):\n",
        "  for r in remove_arr:\n",
        "    text = text.replace(r, new_string)\n",
        "  return text"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNhzmvfS8e2z"
      },
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JecYOGJQ9LkW"
      },
      "source": [
        "# #Tag Wordnet position\n",
        "# wn_pos = [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
        "# print(wn_pos)\n",
        "# print(type(wn_pos))\n",
        "# wnl = WordNetLemmatizer()\n",
        "# lemmatized=\" \".join([wnl.lemmatize(word, tag) for word, tag in wn_pos])\n",
        "# print(lemmatized)\n",
        "# print(type(lemmatized))\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeVFzbhcLEPB"
      },
      "source": [
        "def Lemmatize(text):\n",
        "  #Make Lower Case\n",
        "  text=text.lower()\n",
        "\n",
        "  #Remove Word contractions\n",
        "  text = remove_contractions(text)\n",
        "\n",
        "  #Remove Punctuation\n",
        "  text = remove_punctuations(text)\n",
        "\n",
        "\n",
        "  #Replace Strings\n",
        "\n",
        "  #Replace all days of week by string \"dayofweek\"\n",
        "  weekstring =['monday', 'tuesday', 'wednesday', 'thursday', 'friday','saturday','sunday']\n",
        "  text = replace_strings(text,'dayofweek', weekstring)\n",
        "\n",
        "  #Replace all month names by string \"month\"\n",
        "  months = ['january','february', 'march','april','may','june','july','august', 'september', 'october','november','december']\n",
        "  text = replace_strings(text,'month', months)\n",
        "\n",
        "  #Replace all Numbers by String 'number'\n",
        "  nums=['one','two','three','four','five','six','seven','eight','nine','ten']\n",
        "  text = replace_strings(text, 'number',nums)\n",
        "\n",
        "  #Replace all family relations with string \"famrel\"\n",
        "  famrel=['dad','mum','daughter','son','aunt','uncle','husband','wife', 'brother','sister', 'father','mother']\n",
        "  text = replace_strings(text, 'famrel',famrel)\n",
        "  \n",
        "  #Replace all friendship (non-family) relations with string \"friend\"\n",
        "  friendrel=['boyfriend','girlfriend','colleague'] \n",
        "  text = replace_strings(text, 'friend',friendrel)\n",
        "\n",
        "  #Tokenize\n",
        "  tokens= word_tokenize(text)\n",
        "\n",
        "  #Remove StopWords\n",
        "  tokens = remove_stopwords(tokens, 'stopwords.txt')\n",
        "  #print(tokens)\n",
        "\n",
        "  #Tagging Parts of Speech in the tokenized \n",
        "  pos = nltk.tag.pos_tag(tokens)\n",
        "  #print(pos)\n",
        "\n",
        "  #Tag Wordnet position\n",
        "  wn_pos = [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
        "\n",
        "  #Get Lemmatized String\n",
        "  wnl = WordNetLemmatizer()\n",
        "  lemmatized=\" \".join([wnl.lemmatize(word, tag) for word, tag in wn_pos])\n",
        "\n",
        "  return lemmatized"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn1Fx7_vIwl6"
      },
      "source": [
        "def Vectorize(text, vectorizer):\n",
        "  #n-gram Tokenization\n",
        "  print(text)\n",
        "  vectorized =  vectorizer.transform([text])\n",
        "  print(vectorized)\n",
        "  return vectorized\n",
        "\n",
        "  "
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB2Q256GJcYR"
      },
      "source": [
        "vectorizer=joblib.load(\"EAFC_vectorizer.joblib\")"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZJTF2pyasRo"
      },
      "source": [
        "model = joblib.load(\"EAFC_TC.joblib\")"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI8VCagUJxEE"
      },
      "source": [
        "text = Lemmatize(text)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogDGOwpDKJVK",
        "outputId": "ced3016f-6a5d-44e5-e930-66380f7d4731"
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "receive result yesterday cancer remission yay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T78O6Q59bO2K",
        "outputId": "99f1f358-8952-4b55-cdcf-605dad233310"
      },
      "source": [
        "text_vectorized = Vectorize(text, vectorizer)\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "receive result yesterday cancer remission yay\n",
            "  (0, 7965)\t0.38236320007347296\n",
            "  (0, 7911)\t0.6227682700968598\n",
            "  (0, 5936)\t0.34476600151730785\n",
            "  (0, 5904)\t0.4017019338124211\n",
            "  (0, 5850)\t0.4112439899413921\n",
            "  (0, 1102)\t0.12887360589488803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhobvxU5bMAS"
      },
      "source": [
        "y_pred=model.predict(text_vectorized)\n",
        "if y_pred[0]==0:\n",
        "  result = \"Class 0: Newly Discovered\"\n",
        "elif y_pred[0]==1:\n",
        "  result = \"Class 1: Venting/Sharing\"\n",
        "elif y_pred[0]==2:\n",
        "  result = \"Class 2: Reaching out for help\"\n",
        "elif y_pred[0]==3:\n",
        "  result = \"Class 3: Passing Away\"\n",
        "elif y_pred[0]==4:\n",
        "  result = \"Class 4: Remission\""
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuzbZHpBbOeP",
        "outputId": "0e92a003-ee15-4517-df16-b56124d4b5ac"
      },
      "source": [
        "print(result)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class 4: Remission\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}