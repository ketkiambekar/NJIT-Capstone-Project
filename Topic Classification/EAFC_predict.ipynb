{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EAFC_predict.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNoJdKnN188NrqKR2TuA/xQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketkiambekar/NJIT-Capstone-Project/blob/main/Topic%20Classification/EAFC_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2rGgPs41XG0",
        "outputId": "bb36dca1-0187-43e4-c9b2-b4e910d60b37"
      },
      "source": [
        "pip install contractions"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.6/dist-packages (0.0.45)\n",
            "Requirement already satisfied: textsearch in /usr/local/lib/python3.6/dist-packages (from contractions) (0.0.17)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.1.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.6/dist-packages (from textsearch->contractions) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJyJLaFvaZRK"
      },
      "source": [
        "#from sklearn.svm import LinearSVC\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "import string\n",
        "import contractions\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#import text_laundry\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF6tAt96LgXj",
        "outputId": "71caff42-549b-4c6e-e200-dc9e3930d6a8"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjxPOv22UabD"
      },
      "source": [
        "def remove_punctuations(text):\n",
        "    punc = string.punctuation \n",
        "    for punctuation in punc:\n",
        "        text = text.replace(punctuation, '')\n",
        "    return text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJixi2Q5v1Uv"
      },
      "source": [
        "def remove_contractions(text):\n",
        "  return \" \".join([contractions.fix(word) for word in text.split()])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WvaO9Gw3S8O"
      },
      "source": [
        "def remove_stopwords(text, filename):\n",
        "  stop_words=[]\n",
        "  with open(filename,'r') as f:\n",
        "    for line in f:\n",
        "      for word in line.split():\n",
        "        stop_words.append(word.lower()) \n",
        "  result =  [word for word in text if word not in stop_words]\n",
        "  return result"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBegyp53LWN4"
      },
      "source": [
        "\n",
        "# text=\"I am in remission yay\"\n",
        "# tokens= word_tokenize(text)\n",
        "# print(tokens)\n",
        "# tokens = remove_stopwords(tokens, 'stopwords.txt')\n",
        "\n",
        "# print(tokens)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dOEx7rL4cCl"
      },
      "source": [
        "def replace_strings(text, new_string , remove_arr ):\n",
        "  for r in remove_arr:\n",
        "    text = text.replace(r, new_string)\n",
        "  return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNhzmvfS8e2z"
      },
      "source": [
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JecYOGJQ9LkW"
      },
      "source": [
        "# #Tag Wordnet position\n",
        "# wn_pos = [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
        "# print(wn_pos)\n",
        "# print(type(wn_pos))\n",
        "# wnl = WordNetLemmatizer()\n",
        "# lemmatized=\" \".join([wnl.lemmatize(word, tag) for word, tag in wn_pos])\n",
        "# print(lemmatized)\n",
        "# print(type(lemmatized))\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeVFzbhcLEPB"
      },
      "source": [
        "def Lemmatize(text):\n",
        "  #Make Lower Case\n",
        "  text=text.lower()\n",
        "\n",
        "  #Remove Word contractions\n",
        "  text = remove_contractions(text)\n",
        "\n",
        "  #Remove Punctuation\n",
        "  text = remove_punctuations(text)\n",
        "\n",
        "  print(text)\n",
        "  #Replace Strings\n",
        "\n",
        "  #Replace all days of week by string \"dayofweek\"\n",
        "  weekstring =['monday', 'tuesday', 'wednesday', 'thursday', 'friday','saturday','sunday']\n",
        "  text = replace_strings(text,'dayofweek', weekstring)\n",
        "\n",
        "  #Replace all month names by string \"month\"\n",
        "  months = ['january','february', 'march','april','may','june','july','august', 'september', 'october','november','december']\n",
        "  text = replace_strings(text,'month', months)\n",
        "\n",
        "  #Replace all Numbers by String 'number'\n",
        "  nums=['one','two','three','four','five','six','seven','eight','nine','ten']\n",
        "  text = replace_strings(text, 'number',nums)\n",
        "\n",
        "  #Replace all family relations with string \"famrel\"\n",
        "  famrel=['dad','mum','daughter','son','aunt','uncle','husband','wife', 'brother','sister', 'father','mother']\n",
        "  text = replace_strings(text, 'famrel',famrel)\n",
        "  \n",
        "  #Replace all friendship (non-family) relations with string \"friend\"\n",
        "  friendrel=['boyfriend','girlfriend','colleague'] \n",
        "  text = replace_strings(text, 'friend',friendrel)\n",
        "\n",
        "  #Tokenize\n",
        "  tokens= word_tokenize(text)\n",
        "\n",
        "  #Remove StopWords\n",
        "  tokens = remove_stopwords(tokens, 'stopwords.txt')\n",
        "  #print(tokens)\n",
        "\n",
        "  #Tagging Parts of Speech in the tokenized \n",
        "  pos = nltk.tag.pos_tag(tokens)\n",
        "  #print(pos)\n",
        "\n",
        "  #Tag Wordnet position\n",
        "  wn_pos = [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in pos]\n",
        "\n",
        "  #Get Lemmatized String\n",
        "  wnl = WordNetLemmatizer()\n",
        "  lemmatized=\" \".join([wnl.lemmatize(word, tag) for word, tag in wn_pos])\n",
        "\n",
        "  return lemmatized"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn1Fx7_vIwl6"
      },
      "source": [
        "def Vectorize(text, vectorizer):\n",
        "  #n-gram Tokenization\n",
        "  print(text)\n",
        "  vectorized =  vectorizer.transform([text])\n",
        "  print(vectorized)\n",
        "  return vectorized\n",
        "\n",
        "  "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB2Q256GJcYR"
      },
      "source": [
        "vectorizer=joblib.load(\"EAFC_vectorizer.joblib\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZJTF2pyasRo"
      },
      "source": [
        "model = joblib.load(\"EAFC_TC.joblib\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItKgS6lgJf-3"
      },
      "source": [
        "text=\"My sister passed away last night\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI8VCagUJxEE",
        "outputId": "ffad63e9-499f-415a-d30e-bf9ba135a568"
      },
      "source": [
        "text = Lemmatize(text)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "my sister passed away last night\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogDGOwpDKJVK",
        "outputId": "4f3fd7f9-3cf3-46aa-85a7-5878cbc87e88"
      },
      "source": [
        "print(text)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "famrel pass away last night\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T78O6Q59bO2K",
        "outputId": "c8e3b04f-fbed-4207-801a-ec454d9e2ab5"
      },
      "source": [
        "text_vectorized = Vectorize(text, vectorizer)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "famrel pass away last night\n",
            "  (0, 5572)\t0.39081284256860926\n",
            "  (0, 5570)\t0.24751729611641304\n",
            "  (0, 5569)\t0.20597523315673488\n",
            "  (0, 4867)\t0.22526293644794732\n",
            "  (0, 4016)\t0.29985026412157173\n",
            "  (0, 3995)\t0.14914697897111165\n",
            "  (0, 2504)\t0.3317406963257357\n",
            "  (0, 2503)\t0.30580848144688594\n",
            "  (0, 2419)\t0.10945634057256315\n",
            "  (0, 473)\t0.43080142668952076\n",
            "  (0, 472)\t0.38127106150762824\n",
            "  (0, 461)\t0.18484431765570467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhobvxU5bMAS"
      },
      "source": [
        "result=model.predict(text_vectorized)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuzbZHpBbOeP",
        "outputId": "b39fcf3c-9c02-45ff-ff51-dac88b11759f"
      },
      "source": [
        "print(result[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}